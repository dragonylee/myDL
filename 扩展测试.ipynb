{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "import time\n",
    "\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 获取数据集\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  torch.Size([60000, 28, 28])\n",
      "Test data:  torch.Size([10000, 28, 28])\n",
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data: \", training_data.data.shape)\n",
    "print(\"Test data: \", test_data.data.shape)\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# 训练方式 CPU/CUDA\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 自定义卷积层\n",
    "\n",
    "class MyConv2dFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def conv2d(input: Tensor, kernel: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "            卷积运算\n",
    "            Output = Input * Kernel\n",
    "        :param input: Tensor[B, Cin, N, N]\n",
    "        :param kernel: Tensor[Cout, Cin, K, K]\n",
    "        :return: Tensor[B, Cout, M, M], M=N-K+1\n",
    "        \"\"\"\n",
    "        B = input.shape[0]\n",
    "        Cin = input.shape[1]\n",
    "        N = input.shape[2]\n",
    "        Cout = kernel.shape[0]\n",
    "        K = kernel.shape[2]\n",
    "        M = N - K + 1\n",
    "\n",
    "        input_unf = nn.Unfold(kernel_size=K)(input)\n",
    "        input_unf = input_unf.view((B, Cin, -1, M, M))\n",
    "        kernel_view = kernel.view((Cout, Cin, K * K))\n",
    "\n",
    "        output = torch.einsum(\"ijklm,njk->inlm\", input_unf, kernel_view)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight):\n",
    "        ctx.save_for_backward(input, weight)\n",
    "        output = MyConv2dFunc.conv2d(input, weight)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight = ctx.saved_tensors\n",
    "        grad_input = grad_weight = None\n",
    "        if grad_output is None:\n",
    "            return None, None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            # 反卷积\n",
    "            gop = nn.ZeroPad2d(weight.shape[2] - 1)(grad_output)\n",
    "            kk = torch.rot90(weight, 2, (2, 3))  # 旋转180度\n",
    "            kk = torch.transpose(kk, 0, 1)\n",
    "            grad_input = MyConv2dFunc.conv2d(gop, kk)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            input_ = torch.transpose(input, 0, 1)\n",
    "            grad_output_ = torch.transpose(grad_output, 0, 1)\n",
    "            grad_weight = MyConv2dFunc.conv2d(input_, grad_output_).transpose(0, 1)\n",
    "        return grad_input, grad_weight\n",
    "\n",
    "\n",
    "class MyConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size: tuple):\n",
    "        super(MyConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        # Parameters\n",
    "        self.weight = nn.Parameter(torch.empty(out_channels, in_channels, kernel_size[0], kernel_size[1]))\n",
    "\n",
    "        nn.init.uniform_(self.weight, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return MyConv2dFunc.apply(x, self.weight)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'MyConv2d: in_channels={}, out_channels={}, kernel_size={}'.format(\n",
    "            self.in_channels, self.out_channels, self.kernel_size\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 用现有的卷积层定义模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 12 * 12, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 用自定义的卷积层定义模型\n",
    "\n",
    "class CNN_new(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_new, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            MyConv2d(1, 32, (3, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (3, 3)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 12 * 12, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 实例1\n",
    "model = CNN().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_new(\n",
      "  (layer1): Sequential(\n",
      "    (0): MyConv2d(MyConv2d: in_channels=1, out_channels=32, kernel_size=(3, 3))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 实例2\n",
    "model_new = CNN_new().to(device)\n",
    "print(model_new)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# 训练和测试函数\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.001021  [    0/60000]\n",
      "loss: 0.002670  [ 6400/60000]\n",
      "loss: 0.000270  [12800/60000]\n",
      "loss: 0.000347  [19200/60000]\n",
      "loss: 0.000355  [25600/60000]\n",
      "loss: 0.000165  [32000/60000]\n",
      "loss: 0.000091  [38400/60000]\n",
      "loss: 0.002655  [44800/60000]\n",
      "loss: 0.004224  [51200/60000]\n",
      "loss: 0.000110  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.035867 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.000370  [    0/60000]\n",
      "loss: 0.000260  [ 6400/60000]\n",
      "loss: 0.000396  [12800/60000]\n",
      "loss: 0.000471  [19200/60000]\n",
      "loss: 0.000312  [25600/60000]\n",
      "loss: 0.000129  [32000/60000]\n",
      "loss: 0.000069  [38400/60000]\n",
      "loss: 0.000587  [44800/60000]\n",
      "loss: 0.001002  [51200/60000]\n",
      "loss: 0.000298  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.033667 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.000909  [    0/60000]\n",
      "loss: 0.000534  [ 6400/60000]\n",
      "loss: 0.000492  [12800/60000]\n",
      "loss: 0.000298  [19200/60000]\n",
      "loss: 0.000330  [25600/60000]\n",
      "loss: 0.000103  [32000/60000]\n",
      "loss: 0.000076  [38400/60000]\n",
      "loss: 0.000386  [44800/60000]\n",
      "loss: 0.001262  [51200/60000]\n",
      "loss: 0.001326  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.033941 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.001160  [    0/60000]\n",
      "loss: 0.000542  [ 6400/60000]\n",
      "loss: 0.000381  [12800/60000]\n",
      "loss: 0.000204  [19200/60000]\n",
      "loss: 0.000326  [25600/60000]\n",
      "loss: 0.000098  [32000/60000]\n",
      "loss: 0.000077  [38400/60000]\n",
      "loss: 0.000382  [44800/60000]\n",
      "loss: 0.000713  [51200/60000]\n",
      "loss: 0.000517  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.032734 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.000287  [    0/60000]\n",
      "loss: 0.000473  [ 6400/60000]\n",
      "loss: 0.000396  [12800/60000]\n",
      "loss: 0.000205  [19200/60000]\n",
      "loss: 0.000335  [25600/60000]\n",
      "loss: 0.000098  [32000/60000]\n",
      "loss: 0.000075  [38400/60000]\n",
      "loss: 0.000380  [44800/60000]\n",
      "loss: 0.000596  [51200/60000]\n",
      "loss: 0.000392  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.031759 \n",
      "\n",
      "Done!  Time cost:  33.72303128242493\n"
     ]
    }
   ],
   "source": [
    "######## 训练第一个模型 ###########\n",
    "\n",
    "# 损失函数 和 优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# 训练\n",
    "epochs = 5\n",
    "print(\"start!\")\n",
    "t1 = time.time()\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!  Time cost: \", time.time() - t1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.334300  [    0/60000]\n",
      "loss: 0.147311  [ 6400/60000]\n",
      "loss: 0.157568  [12800/60000]\n",
      "loss: 0.047446  [19200/60000]\n",
      "loss: 0.047818  [25600/60000]\n",
      "loss: 0.186212  [32000/60000]\n",
      "loss: 0.122187  [38400/60000]\n",
      "loss: 0.094942  [44800/60000]\n",
      "loss: 0.159324  [51200/60000]\n",
      "loss: 0.076851  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.060762 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.031384  [    0/60000]\n",
      "loss: 0.149726  [ 6400/60000]\n",
      "loss: 0.034975  [12800/60000]\n",
      "loss: 0.013555  [19200/60000]\n",
      "loss: 0.002692  [25600/60000]\n",
      "loss: 0.057804  [32000/60000]\n",
      "loss: 0.057752  [38400/60000]\n",
      "loss: 0.036334  [44800/60000]\n",
      "loss: 0.096239  [51200/60000]\n",
      "loss: 0.037418  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.046696 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.011701  [    0/60000]\n",
      "loss: 0.094357  [ 6400/60000]\n",
      "loss: 0.026394  [12800/60000]\n",
      "loss: 0.048856  [19200/60000]\n",
      "loss: 0.019533  [25600/60000]\n",
      "loss: 0.067598  [32000/60000]\n",
      "loss: 0.039261  [38400/60000]\n",
      "loss: 0.016433  [44800/60000]\n",
      "loss: 0.068729  [51200/60000]\n",
      "loss: 0.017940  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031886 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.001247  [    0/60000]\n",
      "loss: 0.039716  [ 6400/60000]\n",
      "loss: 0.027108  [12800/60000]\n",
      "loss: 0.006267  [19200/60000]\n",
      "loss: 0.005124  [25600/60000]\n",
      "loss: 0.013930  [32000/60000]\n",
      "loss: 0.005314  [38400/60000]\n",
      "loss: 0.010639  [44800/60000]\n",
      "loss: 0.074050  [51200/60000]\n",
      "loss: 0.022039  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.033032 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.001653  [    0/60000]\n",
      "loss: 0.012405  [ 6400/60000]\n",
      "loss: 0.013964  [12800/60000]\n",
      "loss: 0.004538  [19200/60000]\n",
      "loss: 0.000905  [25600/60000]\n",
      "loss: 0.009911  [32000/60000]\n",
      "loss: 0.001333  [38400/60000]\n",
      "loss: 0.005667  [44800/60000]\n",
      "loss: 0.031562  [51200/60000]\n",
      "loss: 0.014275  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.033507 \n",
      "\n",
      "Done!  Time cost:  76.49108266830444\n"
     ]
    }
   ],
   "source": [
    "######## 训练第二个模型new ###########\n",
    "\n",
    "# 损失函数 和 优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_new.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# 训练\n",
    "epochs = 5\n",
    "print(\"start!\")\n",
    "t1 = time.time()\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model_new, loss_fn, optimizer)\n",
    "    test(test_dataloader, model_new, loss_fn)\n",
    "print(\"Done!  Time cost: \", time.time() - t1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}