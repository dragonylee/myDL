{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "# 训练数据，AE不需要标签\n",
    "training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# 训练方式 CPU/CUDA\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# AE模型定义，将(28*28)的图像编码为3个特征\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(12, 3),  # 编码为3个特征\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid(),  # 输出值为(0, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 实例化\n",
    "ae = AE().to(device)\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=0.001)\n",
    "loss_func = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.231657  [    0/60000]\n",
      "loss: 0.066269  [ 6400/60000]\n",
      "loss: 0.064593  [12800/60000]\n",
      "loss: 0.068239  [19200/60000]\n",
      "loss: 0.062330  [25600/60000]\n",
      "loss: 0.071733  [32000/60000]\n",
      "loss: 0.069789  [38400/60000]\n",
      "loss: 0.068686  [44800/60000]\n",
      "loss: 0.072319  [51200/60000]\n",
      "loss: 0.066503  [57600/60000]\n",
      "loss: 0.059758  [    0/60000]\n",
      "loss: 0.062318  [ 6400/60000]\n",
      "loss: 0.059963  [12800/60000]\n",
      "loss: 0.058667  [19200/60000]\n",
      "loss: 0.056237  [25600/60000]\n",
      "loss: 0.055942  [32000/60000]\n",
      "loss: 0.057651  [38400/60000]\n",
      "loss: 0.056127  [44800/60000]\n",
      "loss: 0.056130  [51200/60000]\n",
      "loss: 0.055185  [57600/60000]\n",
      "loss: 0.056638  [    0/60000]\n",
      "loss: 0.058691  [ 6400/60000]\n",
      "loss: 0.056877  [12800/60000]\n",
      "loss: 0.052161  [19200/60000]\n",
      "loss: 0.052411  [25600/60000]\n",
      "loss: 0.047495  [32000/60000]\n",
      "loss: 0.054669  [38400/60000]\n",
      "loss: 0.049947  [44800/60000]\n",
      "loss: 0.048476  [51200/60000]\n",
      "loss: 0.056276  [57600/60000]\n",
      "loss: 0.051349  [    0/60000]\n",
      "loss: 0.049167  [ 6400/60000]\n",
      "loss: 0.050245  [12800/60000]\n",
      "loss: 0.049346  [19200/60000]\n",
      "loss: 0.048059  [25600/60000]\n",
      "loss: 0.050225  [32000/60000]\n",
      "loss: 0.047489  [38400/60000]\n",
      "loss: 0.048510  [44800/60000]\n",
      "loss: 0.050236  [51200/60000]\n",
      "loss: 0.050108  [57600/60000]\n",
      "loss: 0.047965  [    0/60000]\n",
      "loss: 0.047260  [ 6400/60000]\n",
      "loss: 0.047379  [12800/60000]\n",
      "loss: 0.048192  [19200/60000]\n",
      "loss: 0.052334  [25600/60000]\n",
      "loss: 0.049722  [32000/60000]\n",
      "loss: 0.047366  [38400/60000]\n",
      "loss: 0.045035  [44800/60000]\n",
      "loss: 0.045362  [51200/60000]\n",
      "loss: 0.049132  [57600/60000]\n",
      "loss: 0.043813  [    0/60000]\n",
      "loss: 0.048348  [ 6400/60000]\n",
      "loss: 0.048516  [12800/60000]\n",
      "loss: 0.045073  [19200/60000]\n",
      "loss: 0.044626  [25600/60000]\n",
      "loss: 0.041891  [32000/60000]\n",
      "loss: 0.046976  [38400/60000]\n",
      "loss: 0.041938  [44800/60000]\n",
      "loss: 0.045654  [51200/60000]\n",
      "loss: 0.042122  [57600/60000]\n",
      "loss: 0.045398  [    0/60000]\n",
      "loss: 0.046133  [ 6400/60000]\n",
      "loss: 0.042578  [12800/60000]\n",
      "loss: 0.042957  [19200/60000]\n",
      "loss: 0.041088  [25600/60000]\n",
      "loss: 0.042816  [32000/60000]\n",
      "loss: 0.040817  [38400/60000]\n",
      "loss: 0.040983  [44800/60000]\n",
      "loss: 0.042995  [51200/60000]\n",
      "loss: 0.044193  [57600/60000]\n",
      "loss: 0.042670  [    0/60000]\n",
      "loss: 0.041662  [ 6400/60000]\n",
      "loss: 0.044575  [12800/60000]\n",
      "loss: 0.043606  [19200/60000]\n",
      "loss: 0.040249  [25600/60000]\n",
      "loss: 0.040403  [32000/60000]\n",
      "loss: 0.039028  [38400/60000]\n",
      "loss: 0.040960  [44800/60000]\n",
      "loss: 0.039359  [51200/60000]\n",
      "loss: 0.038093  [57600/60000]\n",
      "loss: 0.042182  [    0/60000]\n",
      "loss: 0.044105  [ 6400/60000]\n",
      "loss: 0.039995  [12800/60000]\n",
      "loss: 0.041017  [19200/60000]\n",
      "loss: 0.039095  [25600/60000]\n",
      "loss: 0.043384  [32000/60000]\n",
      "loss: 0.039425  [38400/60000]\n",
      "loss: 0.036765  [44800/60000]\n",
      "loss: 0.035625  [51200/60000]\n",
      "loss: 0.041067  [57600/60000]\n",
      "loss: 0.045061  [    0/60000]\n",
      "loss: 0.041724  [ 6400/60000]\n",
      "loss: 0.036242  [12800/60000]\n",
      "loss: 0.038138  [19200/60000]\n",
      "loss: 0.037244  [25600/60000]\n",
      "loss: 0.034249  [32000/60000]\n",
      "loss: 0.039075  [38400/60000]\n",
      "loss: 0.035193  [44800/60000]\n",
      "loss: 0.034442  [51200/60000]\n",
      "loss: 0.037605  [57600/60000]\n",
      "loss: 0.037650  [    0/60000]\n",
      "loss: 0.036489  [ 6400/60000]\n",
      "loss: 0.037767  [12800/60000]\n",
      "loss: 0.037094  [19200/60000]\n",
      "loss: 0.037856  [25600/60000]\n",
      "loss: 0.036099  [32000/60000]\n",
      "loss: 0.037791  [38400/60000]\n",
      "loss: 0.040051  [44800/60000]\n",
      "loss: 0.035884  [51200/60000]\n",
      "loss: 0.035599  [57600/60000]\n",
      "loss: 0.037557  [    0/60000]\n",
      "loss: 0.039680  [ 6400/60000]\n",
      "loss: 0.037688  [12800/60000]\n",
      "loss: 0.036439  [19200/60000]\n",
      "loss: 0.036066  [25600/60000]\n",
      "loss: 0.035616  [32000/60000]\n",
      "loss: 0.035919  [38400/60000]\n",
      "loss: 0.037618  [44800/60000]\n",
      "loss: 0.036011  [51200/60000]\n",
      "loss: 0.034638  [57600/60000]\n",
      "loss: 0.036457  [    0/60000]\n",
      "loss: 0.033020  [ 6400/60000]\n",
      "loss: 0.037201  [12800/60000]\n",
      "loss: 0.041056  [19200/60000]\n",
      "loss: 0.034011  [25600/60000]\n",
      "loss: 0.035041  [32000/60000]\n",
      "loss: 0.040275  [38400/60000]\n",
      "loss: 0.037001  [44800/60000]\n",
      "loss: 0.037900  [51200/60000]\n",
      "loss: 0.037525  [57600/60000]\n",
      "loss: 0.032299  [    0/60000]\n",
      "loss: 0.032444  [ 6400/60000]\n",
      "loss: 0.034816  [12800/60000]\n",
      "loss: 0.038716  [19200/60000]\n",
      "loss: 0.038583  [25600/60000]\n",
      "loss: 0.033395  [32000/60000]\n",
      "loss: 0.033022  [38400/60000]\n",
      "loss: 0.032157  [44800/60000]\n",
      "loss: 0.034125  [51200/60000]\n",
      "loss: 0.037363  [57600/60000]\n",
      "loss: 0.034254  [    0/60000]\n",
      "loss: 0.035365  [ 6400/60000]\n",
      "loss: 0.035852  [12800/60000]\n",
      "loss: 0.035848  [19200/60000]\n",
      "loss: 0.033756  [25600/60000]\n",
      "loss: 0.035416  [32000/60000]\n",
      "loss: 0.039555  [38400/60000]\n",
      "loss: 0.034667  [44800/60000]\n",
      "loss: 0.032156  [51200/60000]\n",
      "loss: 0.035511  [57600/60000]\n",
      "loss: 0.035265  [    0/60000]\n",
      "loss: 0.032652  [ 6400/60000]\n",
      "loss: 0.035060  [12800/60000]\n",
      "loss: 0.036427  [19200/60000]\n",
      "loss: 0.035411  [25600/60000]\n",
      "loss: 0.036661  [32000/60000]\n",
      "loss: 0.036584  [38400/60000]\n",
      "loss: 0.035913  [44800/60000]\n",
      "loss: 0.031866  [51200/60000]\n",
      "loss: 0.029364  [57600/60000]\n",
      "loss: 0.034018  [    0/60000]\n",
      "loss: 0.034769  [ 6400/60000]\n",
      "loss: 0.035272  [12800/60000]\n",
      "loss: 0.030213  [19200/60000]\n",
      "loss: 0.035882  [25600/60000]\n",
      "loss: 0.035100  [32000/60000]\n",
      "loss: 0.032340  [38400/60000]\n",
      "loss: 0.031618  [44800/60000]\n",
      "loss: 0.033992  [51200/60000]\n",
      "loss: 0.031406  [57600/60000]\n",
      "loss: 0.033233  [    0/60000]\n",
      "loss: 0.032760  [ 6400/60000]\n",
      "loss: 0.034106  [12800/60000]\n",
      "loss: 0.035322  [19200/60000]\n",
      "loss: 0.037028  [25600/60000]\n",
      "loss: 0.031240  [32000/60000]\n",
      "loss: 0.032709  [38400/60000]\n",
      "loss: 0.035487  [44800/60000]\n",
      "loss: 0.034771  [51200/60000]\n",
      "loss: 0.027189  [57600/60000]\n",
      "loss: 0.030805  [    0/60000]\n",
      "loss: 0.035425  [ 6400/60000]\n",
      "loss: 0.036410  [12800/60000]\n",
      "loss: 0.035236  [19200/60000]\n",
      "loss: 0.033308  [25600/60000]\n",
      "loss: 0.036189  [32000/60000]\n",
      "loss: 0.034789  [38400/60000]\n",
      "loss: 0.034032  [44800/60000]\n",
      "loss: 0.032022  [51200/60000]\n",
      "loss: 0.037776  [57600/60000]\n",
      "loss: 0.031404  [    0/60000]\n",
      "loss: 0.033382  [ 6400/60000]\n",
      "loss: 0.032049  [12800/60000]\n",
      "loss: 0.034606  [19200/60000]\n",
      "loss: 0.035540  [25600/60000]\n",
      "loss: 0.030662  [32000/60000]\n",
      "loss: 0.035506  [38400/60000]\n",
      "loss: 0.031094  [44800/60000]\n",
      "loss: 0.036730  [51200/60000]\n",
      "loss: 0.032284  [57600/60000]\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "epochs = 20\n",
    "ae.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch, (x, y) in enumerate(train_dataloader):\n",
    "        x1 = x.view(-1, 28 * 28)\n",
    "        x2 = x.view(-1, 28 * 28)\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "\n",
    "        encoded, decoded = ae(x1)\n",
    "        loss = loss_func(decoded, x2)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(x1)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{len(train_dataloader.dataset):>5d}]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=12, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=12, out_features=3, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=12, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=128, out_features=784, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(ae)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 可视化输入和输出（对比）\n",
    "show_n = 10\n",
    "f, a = plt.subplots(2, show_n, figsize=(show_n, 2))\n",
    "\n",
    "x = training_data.data[:show_n]\n",
    "encoded, decoded = ae(x.view(-1, 28 * 28).type(torch.FloatTensor).to(device))\n",
    "y = np.reshape(decoded.cpu().data.numpy(), (-1, 28, 28))\n",
    "\n",
    "for i in range(show_n):\n",
    "    a[0][i].imshow(x[i], cmap='gray')\n",
    "    a[0][i].set_xticks(())\n",
    "    a[0][i].set_yticks(())\n",
    "    a[1][i].imshow(y[i], cmap='gray')\n",
    "    a[1][i].set_xticks(())\n",
    "    a[1][i].set_yticks(())\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# （AE将原始数据降维到3维）空间显示若干个数据的编码\n",
    "n = 2000\n",
    "show_labels = [0, 5, 7]\n",
    "\n",
    "view_data = training_data.data[:n].view(-1, 28 * 28).type(torch.FloatTensor).to(device)\n",
    "encoded_data, _ = ae(view_data)\n",
    "encoded_data = encoded_data.cpu()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "X, Y, Z = encoded_data.data[:, 0].numpy(), encoded_data.data[:, 1].numpy(), encoded_data.data[:, 2].numpy()\n",
    "values = training_data.targets[:n].numpy()\n",
    "\n",
    "for x, y, z, s in zip(X, Y, Z, values):\n",
    "    c = matplotlib.cm.rainbow(int(255 * s / 9))\n",
    "    if s in show_labels:\n",
    "        ax.text(x, y, z, s, backgroundcolor=c)\n",
    "ax.set_xlim(X.min(), X.max())\n",
    "ax.set_ylim(Y.min(), Y.max())\n",
    "ax.set_zlim(Z.min(), Z.max())\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}